**Scanning --> Parsing --> Analysis**

## Scanning
Scanning, or lexical analysis is the process of a lexer taking in a linear stream of characters and chunks them together into a series of tokens. Some tokens are single characters, like `(` and `,`. Others are several characters long like numbers (`123`), string literals (`hi`), and identifiers (`min`).

ex. 
input: `var average = (min+max)/2;`
output `[var] [average] = [(] [min] [+] [max] [)] [/] [2] [;]`

## Parsing
Parsers take in a flat sequence of tokens and builds a parse tree, or abstract syntax tree, out of them. These trees have a structure that mirrors the nested structure of the grammar.

![[Pasted image 20251126013105.png]]

Parsers also report syntax errors.

## Static analysis
The first bit of analysis that most languages do is called **binding** or **resolution**. For each identifier, we find out where that name is defined and we wire them together. This is also where scope comes into play, the region of source code where a certain variable name can refer to a certain declaration. If the language is statically typed, this is also where we type check. Everything up to this point is called the **front end** of the implementation.

## Intermediate representations
Compilers can be thought of as a pipeline where each stage's job is to organize the data representing the user's code in a way that makes the next stage simpler to implement. The front end of the pipeline is specific to the source language that the program is written in, while the back end is more focused on the final architecture where the program will run. 

In the middle, the code may be stored in some **intermediate representation** that isn't tightly tied to either the source or destination. This allows for compatibility between many different source languages and target platforms.

## Optimization
Once we understand what the user's program means, we are free to swap it out with a different program that has the same semantics but implements them more efficiently. A simple example being constant folding: if some expression always evaluates to the exact same value, we can do the evaluation at compile time and replace the code for the expression with its result. If the user typed this:

`penny=3.14159*(0.75/2)*(0.75/2);`

we can do all that arithmetic in the compiler and change the code to:

`pennyArea=0.4417860938;`

## Code generation
The last step in this process after all optimizations have been applied is converting it to a form the machine can actually run. In other words, **generating code**, where "code" refers to assembly-like instructions the CPU can actually run. This is the **back end.** 

This is also where the decision is made to generate instructions for a real CPU or a virtual one. A real CPU can run native code that is very fast, but generating native code is a lot of work. This also means your compiler is tied to a specific architecture. To get around this, you may want to create a compiler that produces **virtual machine code**, often called **bytecode.**

## Virtual Machine
If your compiler produces bytecode, your work isn't over. No chip speaks bytecode, so you then need to translate. This leads to another choice, where you can either write a mini compiler for each architecture you intend to run the code on, or create a virtual machine that emulates a hypothetical chip supporting your virtual architecture at runtime. Running bytecode in a VM is slower than pre-translating it to native code ahead of time because every instruction needs to be simulated at runtime each time it executes. In return though you'll get simplicity and portability.

Ex. Implement your VM in C, and you can run your language on any platform that has a C compiler.

## Runtime
All low-level languages carry out some services while the program is running. For example, if the language automatically manages memory, we need a garbage collector going in order to reclaim unused bits. All of this stuff is going on at runtime, so it's called, appropriately, the **runtime.**

## Tree-walk interpreters
Some programming languages start executing code right after parsing it to n AST. To run the program, the interpreter traverses the syntax tree one branch at a time and evaluates each node as it goes.
## Transpilers
Transpilers allow you to write a front end for your language, and then in the back end, instead of doing all the work to lower the semantics to a primitive target language, you produce a string of source code for some other language that's at about the same level then use that language to get to something you can execute. A good example of this is Web browsers being the "machines" of today, with their "machine code" being JavaScript. This is why almost every programing language has a compiler that targets JavaScript as it's the main way to get code running in the browser.

## Compilers and Interpreters
* When we say a language implementation is a "compiler", we mean it translates source code to some other form but doesn't execute it. The user has to take the resulting output and run it themselves. 
* When we say a language is an "interpreter", we mean it takes in source code and executes it immediately. It runs programs "from the source."

Some languages can be both. From the users perspective they run code from the source, but under the hood they compile the code then run it. In this way, they are both, as they are an interpreter, and they have a compiler.